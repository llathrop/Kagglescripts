{"cells":[
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "#MNIST with sklearn\n\nWe'll run through MNIST using the sklearn library."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# These are all the modules we'll be using later. Make sure you can import them\n# before proceeding further\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nimport sys\nimport time\nimport random\nimport tarfile\nfrom IPython.display import display, Image\n\nfrom scipy import ndimage\nfrom sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score,accuracy_score\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output.\n\n# Config the matlotlib backend as plotting inline in IPython\n%matplotlib inline"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "##Function definitions"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "def loadData(filename):\n    # Load the wholesale customers dataset\n    #data = pd.read_csv(filename)\n    try:\n        data = pd.read_csv(filename, parse_dates=True)\n        #data.drop(['Region', 'Channel'], axis = 1, inplace = True)\n        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n    except Exception as e:\n        print (\"Dataset could not be loaded. Is the dataset missing?\")\n        print(e)\n    return data\ndef writeData(data,filename):\n    # Load the wholesale customers dataset\n    try:\n        data.to_csv(filename, index=False)\n    except Exception as e:\n        print (\"Dataset could not be written.\")\n        print(e)\n    verify=[]\n    try:\n        with open(filename, 'r') as f:\n            for line in f:\n                verify.append(line)\n        f.closed\n        return verify[:5]\n    except IOError:\n        sys.std\n\ndef dispImage(image):\n        plt.imshow(image, cmap='binary')\n        plt.show()\n    \ndef runPredict(clf,Data, display=True):\n    index=random.randrange(len(Data))\n    y_pred = clf.predict(Data[index].reshape(1, -1))[0]\n    if display==True:\n        dispImage(np.reshape(Data[index],(28,28)))\n    return y_pred\n\ndef train_classifier(clf, X_train, y_train):\n    start = time.time()\n    clf.fit(X_train, y_train)\n    end = time.time()\n    return end - start\n    #print \"Done!\\nTraining time (secs): {:.3f}\".format(end - start)\n    \n# Predict on training set and compute F1 score\ndef predict_labels(clf, features, target):\n    #print \"Predicting labels using {}...\".format(clf.__class__.__name__)\n    start = time.time()\n    y_pred = clf.predict(features)\n    end = time.time()\n    #print \"Done!\\nPrediction time (secs): {:.3f}\".format(end - start)\n    return f1_score(target, y_pred,average='micro'),end - start #(None, 'micro', 'macro', 'weighted', 'samples')\n\n# Train and predict using different training set sizes\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n\n    timeTrain=train_classifier(clf, X_train, y_train)\n    predict_train,trainDelta=predict_labels(clf, X_train, y_train)\n    predict_test,testDelta=predict_labels(clf, X_test, y_test)\n    return predict_test,testDelta,predict_train,trainDelta,timeTrain # let's return the scores, so we can use them for comparisons\n\n#for each data set size run and plot a train/test\ndef runTests(test_sizes, train_dataset,train_labels,test_dataset,test_labels, clf=\"\"):\n    test_f1=[]\n    train_f1=[]\n\n    for test_size in test_sizes:\n        # Set up the train set for the test size\n        X_train=train_dataset[:test_size]\n        y_train=train_labels[:test_size]\n        # Same for test\n        X_test=test_dataset[-test_size:]\n        y_test=test_labels[-test_size:]\n\n        # logistic regresion needs some data massaging to work\n      #  X_train=X_train.reshape(X_train.shape[0],X_train.shape[1]*X_train.shape[2])\n      #  X_test=X_test.reshape(X_test.shape[0],X_test.shape[1]*X_test.shape[2])\n\n        if clf == \"\":\n            clf=LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42,  max_iter=1000,C=1e-5)\n\n        # Fit model to training data\n        test,testDelta,train,trainDelta,timeTrain = train_predict(clf, X_train, y_train, X_test, y_test)\n        test_f1.append(test)\n        train_f1.append(train)\n        print (\"------------------------------------------\")\n        print (\"Training set size: {},\".format(len(X_train)),\"Train time (secs): {:.3f}\".format(timeTrain))\n        print (\"F1 score for training set: {},\".format(train),\"Prediction time (secs): {:.3f}\".format(trainDelta))\n        print (\"F1 score for test set: {},\".format(test),\"Prediction time (secs): {:.3f}\".format(testDelta))\n\n    \n    print (\"\\n\",clf)\n    print(\"Test F1:{}\".format(test_f1))\n    display(\"Train F1:{}\".format(train_f1))\n    plt.plot(test_f1,label=\"Test F1\")\n    plt.plot(train_f1,label=\"Train F1\")\n    plt.legend(loc=2)\n    plt.title(\"F1 Score per run\")\n    plt.show()\n    \n    return clf    "
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "## Load the Training data\nWe'll load the training data and take a look at a row, verify features, etc. We'll also display a data point as an image and verify it matches the label"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#Load up the train data\ntrainData=loadData(\"../input/train.csv\")\nprint (trainData.head(1))\ny = trainData[\"label\"]\nx = trainData.drop(\"label\", axis=1)\n#print (y.head(2))\n#print (x.head(2))\n\n#print (x.values[5])\nprint (\"size of each entry\",len(x.values[5]))\n\nindex=random.randrange(len(x))\nprint(\"for index\",index,\"label is:\",y.values[index])\ndispImage(np.reshape(x.values[index],(28,28)))"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "## Split the data for train/test, and run training data of various size through the classifier\nWe use train_test_split, from the sklearn.cross_validation to split the data into a training and validation group. Then we feed this data in various test sizes to a classifier and evaluate the scores. \nFinally, we run a prediction on a random value from the validation set, and visually ensure that it is predicted correctly!"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#  train/validation split\nX_train, X_test, y_train, y_test = train_test_split( x.values, y.values, test_size=0.25, random_state=42)\n\ndataSize=X_train.shape[0]\nprint (\"size of train data\",dataSize)\ntest_sizes=[50]\nfor i in range(5):\n    test_sizes.append(int(round(dataSize*(i+1)*.2)))\n\ntest_sizes=[63,630,6300,31500]\n#test_sizes=[50,500,5001]\nprint (\"run tests of size\",test_sizes)\nclf=runTests(test_sizes, X_train,y_train,X_test,y_test)\n\nprint(\"Validation Prediction is:\",runPredict(clf,X_test))"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "## test data, \nNow, we load the test data and make sure that it also can be used to make a prediction. the output should be correctly predicted or at least reasonably close!"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#loadup the  test data\nprint (\"Test Data:\")\ntestData=loadData(\"../input/test.csv\") # no need to load this yet!print (testData)\n\n#testData = np.array(testData).reshape((len(testData), -1))\n\nprint(\"Test Set Prediction is:\",runPredict(clf,testData.values))"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "## Prep submission \nmake a prediction for each test data point, and store. As we proceed throught the data, we check in to make sure that everything still is being predicted correctly!"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "\nsubmission =[]\nfor index in range(len(testData.values)):\n    submission.append((index+1,clf.predict(testData.values[index].reshape(1, -1))[0]))\n    if index%5000 == 0:\n        print(\"run:\",index,\"entry#:\",submission[index][0], \"predicted:\",submission[index][1])\n        dispImage(np.reshape(testData.values[index],(28,28)))\n        \nprint (\"size of submission\",len(submission))\n\n"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "## submission\n\nwrite the data to a csv for submission!"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#Write our the data for submission\nverify=writeData(pd.DataFrame(submission,columns=[\"ImageId\",\"Label\"]),'submission.csv')\nprint(verify)"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "## Conclusions...\n\nregular statistical machine learning methods, via sklearn may be used to make a very successful prediction, but may not be up to the current state of the art. Deep learning methods, particularly a convolutional neural net, such as leCun or the inception net may perform better "
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "###EOF"
 }
],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}, "nbformat": 4, "nbformat_minor": 0}