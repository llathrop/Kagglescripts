{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv.zip\n",
      "train.csv.zip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,random,math,time\n",
    "import tarfile, zipfile\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "\n",
    "from sklearn import decomposition, datasets, ensemble\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "use_xgb=False #disable \n",
    "\n",
    "from subprocess import check_output\n",
    "datadir=\"./input/\"\n",
    "print(check_output([\"ls\", datadir]).decode(\"utf8\"))\n",
    "\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(datadir,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    #data = pd.read_csv(filename)\n",
    "    data = ''\n",
    "    print (\"loading: \"+datadir+filename)\n",
    "    try:\n",
    "        if zipfile.is_zipfile(datadir+filename):\n",
    "            z = zipfile.ZipFile(datadir+filename)\n",
    "            filename = z.open(filename[:-4])\n",
    "        else:\n",
    "            filename=datadir+filename\n",
    "        data = pd.read_csv(filename, parse_dates=True)  \n",
    "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "        print(e)\n",
    "    return data\n",
    "\n",
    "def writeData(data,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be written.\")\n",
    "        print(e)\n",
    "    verify=[]\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                verify.append(line)\n",
    "        f.closed\n",
    "        return verify[:5]\n",
    "    except IOError:\n",
    "        sys.std\n",
    "        \n",
    "def LabelEncoder(data):\n",
    "    # lifted in parts from:\n",
    "    #https://www.kaggle.com/mmueller/allstate-claims-severity/yet-another-xgb-starter/code\n",
    "    features = data.columns\n",
    "    cats = [feat for feat in features if 'cat' in feat]\n",
    "    for feat in cats:\n",
    "        data[feat] = pd.factorize(data[feat], sort=True)[0]\n",
    "    return data\n",
    "\n",
    "# XGB!\n",
    "\n",
    "def xgbfit(X_train,y_train):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    \n",
    "\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'silent': 1,\n",
    "        'subsample': 0.7,\n",
    "        'learning_rate': 0.075,\n",
    "        'objective': 'reg:linear',\n",
    "        'max_depth': 6,\n",
    "        'num_parallel_tree': 1,\n",
    "        'min_child_weight': 1,\n",
    "        'eval_metric': 'mae',\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    res = xgb.cv(xgb_params, dtrain, num_boost_round=750, nfold=4, seed=42, stratified=False,\n",
    "                 early_stopping_rounds=15, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "    print(\"fit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    best_nrounds = res.shape[0] - 1\n",
    "    cv_mean = res.iloc[-1, 0]\n",
    "    cv_std = res.iloc[-1, 1]\n",
    "    print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n",
    "    # XGB Train!\n",
    "    start_time = time.time()\n",
    "    gbdt = xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "    print(\"Train time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/train.csv.zip\n",
      "Dataset has 188318 samples with 132 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
       "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "3  0.602642   939.85  \n",
       "4  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/test.csv.zip\n",
      "Dataset has 125546 samples with 131 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125546 entries, 0 to 125545\n",
      "Columns: 131 entries, id to cont14\n",
      "dtypes: float64(14), int64(1), object(116)\n",
      "memory usage: 125.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281143</td>\n",
       "      <td>0.466591</td>\n",
       "      <td>0.317681</td>\n",
       "      <td>0.61229</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.38016</td>\n",
       "      <td>0.377724</td>\n",
       "      <td>0.369858</td>\n",
       "      <td>0.704052</td>\n",
       "      <td>0.392562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836443</td>\n",
       "      <td>0.482425</td>\n",
       "      <td>0.443760</td>\n",
       "      <td>0.71330</td>\n",
       "      <td>0.51890</td>\n",
       "      <td>0.60401</td>\n",
       "      <td>0.689039</td>\n",
       "      <td>0.675759</td>\n",
       "      <td>0.453468</td>\n",
       "      <td>0.208045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718531</td>\n",
       "      <td>0.212308</td>\n",
       "      <td>0.325779</td>\n",
       "      <td>0.29758</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.30529</td>\n",
       "      <td>0.245410</td>\n",
       "      <td>0.241676</td>\n",
       "      <td>0.258586</td>\n",
       "      <td>0.297232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.369930</td>\n",
       "      <td>0.342355</td>\n",
       "      <td>0.40028</td>\n",
       "      <td>0.33237</td>\n",
       "      <td>0.31480</td>\n",
       "      <td>0.348867</td>\n",
       "      <td>0.341872</td>\n",
       "      <td>0.592264</td>\n",
       "      <td>0.555955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302678</td>\n",
       "      <td>0.398862</td>\n",
       "      <td>0.391833</td>\n",
       "      <td>0.23688</td>\n",
       "      <td>0.43731</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.359572</td>\n",
       "      <td>0.352251</td>\n",
       "      <td>0.301535</td>\n",
       "      <td>0.825823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9    ...        cont5  \\\n",
       "0   4    A    B    A    A    A    A    A    A    B    ...     0.281143   \n",
       "1   6    A    B    A    B    A    A    A    A    B    ...     0.836443   \n",
       "2   9    A    B    A    B    B    A    B    A    B    ...     0.718531   \n",
       "3  12    A    A    A    A    B    A    A    A    A    ...     0.397069   \n",
       "4  15    B    A    A    A    A    B    A    A    A    ...     0.302678   \n",
       "\n",
       "      cont6     cont7    cont8    cont9   cont10    cont11    cont12  \\\n",
       "0  0.466591  0.317681  0.61229  0.34365  0.38016  0.377724  0.369858   \n",
       "1  0.482425  0.443760  0.71330  0.51890  0.60401  0.689039  0.675759   \n",
       "2  0.212308  0.325779  0.29758  0.34365  0.30529  0.245410  0.241676   \n",
       "3  0.369930  0.342355  0.40028  0.33237  0.31480  0.348867  0.341872   \n",
       "4  0.398862  0.391833  0.23688  0.43731  0.50556  0.359572  0.352251   \n",
       "\n",
       "     cont13    cont14  \n",
       "0  0.704052  0.392562  \n",
       "1  0.453468  0.208045  \n",
       "2  0.258586  0.297232  \n",
       "3  0.592264  0.555955  \n",
       "4  0.301535  0.825823  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = loadData(datadir,'train.csv.zip')\n",
    "display(data.info())\n",
    "display(data.head(5))\n",
    "\n",
    "test_data= loadData(datadir,'test.csv.zip') \n",
    "display(test_data.info())\n",
    "display(test_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('test:', 125546)\n",
      "('combined:', 313864)\n",
      "Pre-Processing done\n",
      "('data:', 188318)\n",
      "('labels:', 188318)\n",
      "('test:', 125546)\n"
     ]
    }
   ],
   "source": [
    "# combine the two frames so we can encode the labels!\n",
    "test_data['loss']=0\n",
    "\n",
    "lengthofData=len(data)\n",
    "lengthoftest_data=len(test_data)\n",
    "\n",
    "print(\"data:\",lengthofData)\n",
    "print(\"test:\",lengthoftest_data)\n",
    "\n",
    "combineddata=pd.concat([data,test_data])\n",
    "lengthofcombined=len(combineddata)\n",
    "print(\"combined:\",lengthofcombined)\n",
    "\n",
    "# the categorical data that we need in a number format\n",
    "combineddata=LabelEncoder(combineddata)\n",
    "\n",
    "# time to split the data back apart!\n",
    "data=combineddata.iloc[:lengthofData].copy()\n",
    "test_data=combineddata.iloc[lengthofData:].copy()\n",
    "test_data.drop(['loss'],1,inplace=True) # didn't have this column before, make it go away!\n",
    "\n",
    "\n",
    "x_test = test_data.copy()\n",
    "x_test.drop(['id'],1,inplace=True)\n",
    "\n",
    "# we don't want the ID columns in X, and of course not loss either\n",
    "x=data.drop(['id','loss'],1)\n",
    "# loss is our label\n",
    "y=data['loss']\n",
    "\n",
    "#minmax scaler\n",
    "scaler= MinMaxScaler() \n",
    "x = scaler.fit_transform(x)\n",
    "x_test_data = scaler.fit_transform(x_test)\n",
    "\n",
    "#display(x[:5])\n",
    "#display(y.head(5))\n",
    "\n",
    "print(\"Pre-Processing done\")\n",
    "print(\"data:\",len(x))\n",
    "print(\"labels:\",len(y))\n",
    "print(\"test:\",len(x_test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking, Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of scikitlearn regressors to use:', 3)\n"
     ]
    }
   ],
   "source": [
    "# OK let's actually do some ML\n",
    "regrList=[] # a list of regressions to use\n",
    "#regrList.append(LinearRegression())\n",
    "regrList.append(ExtraTreesRegressor())\n",
    "regrList.append(Ridge())\n",
    "    \n",
    "regrList.append(RandomForestRegressor(n_estimators=10,\n",
    "                                      #criterion = 'mae',\n",
    "                                      n_jobs =-1, \n",
    "                                      random_state=42))\n",
    "print(\"number of scikitlearn regressors to use:\",len(regrList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into k-folds(divisions). train the regressors on each combination of k-1 folds, and then predict on the held-out fold. Preserve the prediction of each regressor for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n"
     ]
    }
   ],
   "source": [
    "#prepare the fold divisions\n",
    "\n",
    "data_size=x.shape[0]\n",
    "print \"size of train data:\",data_size\n",
    "folds=[]\n",
    "num_folds=5\n",
    "fold_start=0\n",
    "for k in range(num_folds-1):\n",
    "    fold_end=((data_size/num_folds)*(k+1))\n",
    "    folds.append((fold_start,fold_end))\n",
    "    fold_start=fold_end\n",
    "folds.append((fold_start,data_size))\n",
    "print \"folds at:\",folds\n",
    "print \"fold size:\", (data_size/num_folds)\n",
    "print \"train size:\",(data_size/num_folds)*(num_folds-1)\n",
    "\n",
    "count=0\n",
    "for i in folds:\n",
    "    count+=i[1]-i[0]\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fold:', 0, 'to', 37663, 'of', 188318)\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "\n",
      "fit time:61.612s\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.5s\n",
      "Mean abs error: 1327.48\n",
      "Score: 0.46\n",
      "\n",
      "fit time:3.545s\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.012s\n",
      "Mean abs error: 1335.61\n",
      "Score: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:24: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:25.83s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "predict time:0.241s\n",
      "Mean abs error: 1328.68\n",
      "Score: 0.46\n",
      "--layer2 length: 37663\n",
      "--layer2 shape: (37663, 3)\n",
      "Fold run time:92.621s\n",
      "('Fold:', 37663, 'to', 75326, 'of', 188318)\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "\n",
      "fit time:61.45s\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.498s\n",
      "Mean abs error: 1309.37\n",
      "Score: 0.49\n",
      "\n",
      "fit time:3.638s\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.012s\n",
      "Mean abs error: 1322.17\n",
      "Score: 0.48\n",
      "\n",
      "fit time:25.66s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "predict time:0.215s\n",
      "Mean abs error: 1302.42\n",
      "Score: 0.49\n",
      "--layer2 length: 75326\n",
      "--layer2 shape: (75326, 3)\n",
      "Fold run time:92.322s\n",
      "('Fold:', 75326, 'to', 112989, 'of', 188318)\n",
      "\n",
      "folding! len test 37663, len train 150655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:51: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:59.024s\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.509s\n",
      "Mean abs error: 1325.27\n",
      "Score: 0.48\n",
      "\n",
      "fit time:3.576s\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.013s\n",
      "Mean abs error: 1330.79\n",
      "Score: 0.48\n",
      "\n",
      "fit time:24.735s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "predict time:0.226s\n",
      "Mean abs error: 1317.28\n",
      "Score: 0.48\n",
      "--layer2 length: 112989\n",
      "--layer2 shape: (112989, 3)\n",
      "Fold run time:88.937s\n",
      "('Fold:', 112989, 'to', 150652, 'of', 188318)\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "\n",
      "fit time:59.472s\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.491s\n",
      "Mean abs error: 1331.13\n",
      "Score: 0.48\n",
      "\n",
      "fit time:3.494s\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.011s\n",
      "Mean abs error: 1338.83\n",
      "Score: 0.48\n",
      "\n",
      "fit time:25.223s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "predict time:0.213s\n",
      "Mean abs error: 1323.89\n",
      "Score: 0.49\n",
      "--layer2 length: 150652\n",
      "--layer2 shape: (150652, 3)\n",
      "Fold run time:89.728s\n",
      "('Fold:', 150652, 'to', 188318, 'of', 188318)\n",
      "\n",
      "folding! len test 37666, len train 150652\n",
      "\n",
      "fit time:61.258s\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.528s\n",
      "Mean abs error: 1307.40\n",
      "Score: 0.49\n",
      "\n",
      "fit time:3.839s\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.012s\n",
      "Mean abs error: 1326.81\n",
      "Score: 0.48\n",
      "\n",
      "fit time:26.334s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "predict time:0.222s\n",
      "Mean abs error: 1311.63\n",
      "Score: 0.50\n",
      "--layer2 length: 188318\n",
      "--layer2 shape: (188318, 3)\n",
      "Fold run time:93.087s\n",
      "Full run time:456.697s\n"
     ]
    }
   ],
   "source": [
    "x_layer2=[]\n",
    "start_time0 = time.time()\n",
    "MAE_tracking=[]\n",
    "\n",
    "for fold_start,fold_end in folds:\n",
    "    print(\"Fold:\",fold_start,\"to\",fold_end,\"of\",data_size)\n",
    "    start_time1 = time.time()\n",
    "    fold_result=[]\n",
    "    \n",
    "    X_test = x[fold_start:fold_end].copy()\n",
    "    y_test = y[fold_start:fold_end].copy()\n",
    "    X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n",
    "    y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "    print \"\\nfolding! len test {}, len train {}\".format(len(X_test),len(X_train))\n",
    "    \n",
    "    for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "        start_time = time.time()\n",
    "        regrList[i].fit(X_train,y_train)\n",
    "        print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "        start_time = time.time()\n",
    "        print(regrList[i])\n",
    "        curr_predict=regrList[i].predict(X_test)\n",
    "        if fold_result == []:\n",
    "            fold_result = np.array(curr_predict.copy())\n",
    "        else:\n",
    "            fold_result = np.column_stack((fold_result,curr_predict))\n",
    "        \n",
    "        print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "        #show some stats on that last regressions run\n",
    "        MAE=np.mean(abs(curr_predict - y_test))\n",
    "        MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,i),MAE])\n",
    "        print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "        print(\"Score: {:.2f}\".format(regrList[i].score(X_test, y_test)))\n",
    "    \n",
    "    #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "    \n",
    "    if use_xgb == True:\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "        gbdt=xgbfit(X_train,y_train)\n",
    "\n",
    "        # now do a prediction and spit out a score(MAE) that means something\n",
    "        start_time = time.time()\n",
    "        curr_predict=gbdt.predict(dtest)\n",
    "        fold_result = np.column_stack((fold_result,curr_predict))   \n",
    "        MAE=np.mean(abs(curr_predict - y_test))\n",
    "        MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,'XGB'),MAE])\n",
    "        print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "        print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    if x_layer2 == []:\n",
    "        x_layer2=fold_result\n",
    "    else:\n",
    "        x_layer2=np.append(x_layer2,fold_result,axis=0)\n",
    "        \n",
    "    print \"--layer2 length:\",len(x_layer2)\n",
    "    print \"--layer2 shape:\",np.shape(x_layer2)\n",
    "    print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \n",
    "print(\"Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### train layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "Mean abs error: 1238.64\n",
      "Score: 0.52\n"
     ]
    }
   ],
   "source": [
    "print len(x_layer2)\n",
    "print len(y)\n",
    "\n",
    "#  train/validation split\n",
    "X_layer2_train, X_layer2_validation, y_layer2_train, y_layer2_validation = train_test_split( x_layer2,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)\n",
    "layer2_regr=LinearRegression()\n",
    "\n",
    "layer2_regr.fit(X_layer2_train,y_layer2_train)\n",
    "\n",
    "layer2_predict=layer2_regr.predict(X_layer2_validation)\n",
    "\n",
    "#show some stats on that last regressions run    \n",
    "MAE=np.mean(abs(layer2_predict - y_layer2_validation))\n",
    "MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"Score: {:.2f}\".format(layer2_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "\n",
    "\n",
    "#with LinearReg: Mean abs error: 1238.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "[0]\ttrain-mae:2810.91+3.90114\ttest-mae:2810.93+12.0785\n",
      "fit time:6.259s\n",
      "CV-Mean: 1207.10253925+5.35427884679\n",
      "Train time:1.196s\n",
      "XGB Mean abs error: 1206.63\n",
      "XGB predict time:0.028s\n"
     ]
    }
   ],
   "source": [
    "# The XGB version of layer 2\n",
    "print len(x_layer2)\n",
    "print len(y)\n",
    "\n",
    "#  train/validation split\n",
    "X_layer2_train, X_layer2_validation, y_layer2_train, y_layer2_validation = train_test_split( x_layer2,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "dtest = xgb.DMatrix(X_layer2_validation)\n",
    "layer2_gbdt=xgbfit(X_layer2_train,y_layer2_train)\n",
    "\n",
    "# now do a prediction and spit out a score(MAE) that means something\n",
    "start_time = time.time()\n",
    "MAE=np.mean(abs(layer2_gbdt.predict(dtest) - y_layer2_validation))\n",
    "MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "#with LinearReg: XGB Mean abs error: 1205.77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['run:0-37663:0' 'run:0-37663:1' 'run:0-37663:2' 'run:37663-75326:0'\n",
      "  'run:37663-75326:1' 'run:37663-75326:2' 'run:75326-112989:0'\n",
      "  'run:75326-112989:1' 'run:75326-112989:2' 'run:112989-150652:0'\n",
      "  'run:112989-150652:1' 'run:112989-150652:2' 'run:150652-188318:0'\n",
      "  'run:150652-188318:1' 'run:150652-188318:2' 'run:linearLayer2'\n",
      "  'run:XGBLayer2']\n",
      " ['1327.48386331' '1335.6109609' '1328.67511744' '1309.37028856'\n",
      "  '1322.16612723' '1302.41998256' '1325.27337328' '1330.79485737'\n",
      "  '1317.28013905' '1331.13141281' '1338.83235314' '1323.88590739'\n",
      "  '1307.39994095' '1326.80644028' '1311.62821712' '1238.63516634'\n",
      "  '1206.63054046']]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAFJCAYAAAB5F0HAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5b20419710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "MAE_tracking_graph=np.array(MAE_tracking)\n",
    "\n",
    "print(MAE_tracking_graph.T)\n",
    "\n",
    "plt.plot(MAE_tracking_graph.T[1])\n",
    "plt.xlabel(MAE_tracking_graph.T[0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del MAE_tracking_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict layer 1 on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:2.101s\n",
      "[ 2324.892  1905.922  8792.681 ...,  3283.863  1688.405  3777.716]\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.082s\n",
      "[  1155.44633825   1986.3797948   11490.70006385 ...,   2823.69355514\n",
      "   1076.85379674   4646.9250013 ]\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:9: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time:0.767s\n",
      "[ 2205.797  2107.822  8917.102 ...,  3224.901  1168.459  4035.305]\n",
      "Fold run time:2.955s\n"
     ]
    }
   ],
   "source": [
    "x_layer2_test = []\n",
    "start_time1 = time.time()\n",
    "for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "    start_time = time.time()\n",
    "    print(regrList[i])\n",
    "    curr_predict=regrList[i].predict(x_test_data)\n",
    "    print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    \n",
    "    if x_layer2_test == []:\n",
    "        x_layer2_test = np.array(curr_predict.copy())\n",
    "    else:\n",
    "        x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    print curr_predict\n",
    "\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "if use_xgb == True:\n",
    "    dtest = xgb.DMatrix(x_test_data)\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    #start_time = time.time()\n",
    "    curr_predict=gbdt.predict(dtest)\n",
    "    x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    #print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'size of original test data:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "125546"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Test shape:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(125546, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'train shape:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(188318, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sample of layer2 test:\\n', array([[  2324.892     ,   1155.44633825,   2205.797     ],\n",
      "       [  1905.922     ,   1986.3797948 ,   2107.822     ],\n",
      "       [  8792.681     ,  11490.70006385,   8917.102     ],\n",
      "       [  8104.001     ,   5737.1042642 ,   4967.937     ]]))\n",
      "('x_layer2_test mean:', array([ 3101.84351085,  3056.02085022,  3087.85538827]))\n",
      "('x_layer2 mean:', array([ 3091.75046737,  3037.45612893,  3093.94178225]))\n",
      "('x_layer2_test std:', array([ 2241.72743736,  2016.1297939 ,  2242.43405966]))\n",
      "('x_layer2 std:', array([ 2263.80509937,  2017.89115492,  2266.12894136]))\n",
      "('num outliers:', 10)\n",
      "3091.75046737\n",
      "('num outliers:', 0)\n",
      "3091.75046737\n"
     ]
    }
   ],
   "source": [
    "# some problems noted---fact finding below!\n",
    "display(\"size of original test data:\",len(x_test_data))\n",
    "display(\"Test shape:\",np.shape(x_layer2_test))\n",
    "display(\"train shape:\",np.shape(x_layer2))\n",
    "\n",
    "print(\"sample of layer2 test:\\n\",x_layer2_test[:4])\n",
    "\n",
    "print(\"x_layer2_test mean:\",x_layer2_test.mean( axis=0))\n",
    "print(\"x_layer2 mean:\",x_layer2.mean(axis=0))\n",
    "train_layer2_col0_mean=x_layer2.mean(axis=0)[0]\n",
    "\n",
    "print(\"x_layer2_test std:\",x_layer2_test.std( axis=0)) \n",
    "print(\"x_layer2 std:\",x_layer2.std(axis=0))\n",
    "\n",
    "# notice that column 0(linregresion) has a significantly higher mean and std\n",
    "# here's a hack to not fix that for now! \n",
    "\n",
    "# check which row in column 0 are significantly far from the mean\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "#for each problem child, set them to the average value from the train set, to null the affect some\n",
    "for o in outliers:\n",
    "    problem_column[o[0]]=train_layer2_col0_mean\n",
    "    \n",
    "print(problem_column[o[0]])\n",
    "\n",
    "#check outliers again\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "print(x_layer2_test.T[0][o[0]]) # verify that the change made it all the way to the original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,1799.32765798\\n',\n",
       " '6,1934.04146669\\n',\n",
       " '9,9952.56254374\\n',\n",
       " '12,6255.53860683\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data['loss']=layer2_regr.predict(x_layer2_test)\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the XGB version:\n",
    "dtest = xgb.DMatrix(x_layer2_test)\n",
    "test_data['loss']=layer2_gbdt.predict(dtest)\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack_xgb.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('result std:', id      170098.335649\n",
      "loss      2116.367421\n",
      "dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "#let's have a look at the std of the result, as a cross check\n",
    "print(\"result std:\",result.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
