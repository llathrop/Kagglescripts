{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv.zip\n",
      "train.csv.zip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys,time,random,math,time\n",
    "import tarfile, zipfile\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import decomposition, datasets, ensemble\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "use_xgb=False #disable for speed\n",
    "\n",
    "from subprocess import check_output\n",
    "datadir=\"./input/\"\n",
    "print(check_output([\"ls\", datadir]).decode(\"utf8\"))\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(datadir,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    #data = pd.read_csv(filename)\n",
    "    data = ''\n",
    "    print (\"loading: \"+datadir+filename)\n",
    "    try:\n",
    "        if zipfile.is_zipfile(datadir+filename):\n",
    "            z = zipfile.ZipFile(datadir+filename)\n",
    "            filename = z.open(filename[:-4])\n",
    "        else:\n",
    "            filename=datadir+filename\n",
    "        data = pd.read_csv(filename, parse_dates=True)  \n",
    "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
    "        print(e)\n",
    "    return data\n",
    "\n",
    "def writeData(data,filename):\n",
    "    # Load the wholesale customers dataset\n",
    "    try:\n",
    "        data.to_csv(filename, index=False)\n",
    "    except Exception as e:\n",
    "        print (\"Dataset could not be written.\")\n",
    "        print(e)\n",
    "    verify=[]\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                verify.append(line)\n",
    "        f.closed\n",
    "        return verify[:5]\n",
    "    except IOError:\n",
    "        sys.std\n",
    "        \n",
    "def LabelEncoder(data):\n",
    "    # lifted in parts from:\n",
    "    #https://www.kaggle.com/mmueller/allstate-claims-severity/yet-another-xgb-starter/code\n",
    "    features = data.columns\n",
    "    cats = [feat for feat in features if 'cat' in feat]\n",
    "    for feat in cats:\n",
    "        data[feat] = pd.factorize(data[feat], sort=True)[0]\n",
    "    return data\n",
    "\n",
    "# XGB!\n",
    "\n",
    "def xgbfit(X_train,y_train):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    \n",
    "\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'silent': 1,\n",
    "        'subsample': 0.7,\n",
    "        'learning_rate': 0.075,\n",
    "        'objective': 'reg:linear',\n",
    "        'max_depth': 6,\n",
    "        'num_parallel_tree': 1,\n",
    "        'min_child_weight': 1,\n",
    "        'eval_metric': 'mae',\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    res = xgb.cv(xgb_params, dtrain, num_boost_round=750, nfold=4, seed=42, stratified=False,\n",
    "                 early_stopping_rounds=15, verbose_eval=100, show_stdv=True, maximize=False)\n",
    "    print(\"fit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    best_nrounds = res.shape[0] - 1\n",
    "    cv_mean = res.iloc[-1, 0]\n",
    "    cv_std = res.iloc[-1, 1]\n",
    "    print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n",
    "    # XGB Train!\n",
    "    start_time = time.time()\n",
    "    gbdt = xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "    print(\"Train time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    return gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/train.csv.zip\n",
      "Dataset has 188318 samples with 132 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188318 entries, 0 to 188317\n",
      "Columns: 132 entries, id to loss\n",
      "dtypes: float64(15), int64(1), object(116)\n",
      "memory usage: 189.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
       "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "3  0.602642   939.85  \n",
       "4  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: ./input/test.csv.zip\n",
      "Dataset has 125546 samples with 131 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125546 entries, 0 to 125545\n",
      "Columns: 131 entries, id to cont14\n",
      "dtypes: float64(14), int64(1), object(116)\n",
      "memory usage: 125.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281143</td>\n",
       "      <td>0.466591</td>\n",
       "      <td>0.317681</td>\n",
       "      <td>0.61229</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.38016</td>\n",
       "      <td>0.377724</td>\n",
       "      <td>0.369858</td>\n",
       "      <td>0.704052</td>\n",
       "      <td>0.392562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836443</td>\n",
       "      <td>0.482425</td>\n",
       "      <td>0.443760</td>\n",
       "      <td>0.71330</td>\n",
       "      <td>0.51890</td>\n",
       "      <td>0.60401</td>\n",
       "      <td>0.689039</td>\n",
       "      <td>0.675759</td>\n",
       "      <td>0.453468</td>\n",
       "      <td>0.208045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718531</td>\n",
       "      <td>0.212308</td>\n",
       "      <td>0.325779</td>\n",
       "      <td>0.29758</td>\n",
       "      <td>0.34365</td>\n",
       "      <td>0.30529</td>\n",
       "      <td>0.245410</td>\n",
       "      <td>0.241676</td>\n",
       "      <td>0.258586</td>\n",
       "      <td>0.297232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397069</td>\n",
       "      <td>0.369930</td>\n",
       "      <td>0.342355</td>\n",
       "      <td>0.40028</td>\n",
       "      <td>0.33237</td>\n",
       "      <td>0.31480</td>\n",
       "      <td>0.348867</td>\n",
       "      <td>0.341872</td>\n",
       "      <td>0.592264</td>\n",
       "      <td>0.555955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302678</td>\n",
       "      <td>0.398862</td>\n",
       "      <td>0.391833</td>\n",
       "      <td>0.23688</td>\n",
       "      <td>0.43731</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.359572</td>\n",
       "      <td>0.352251</td>\n",
       "      <td>0.301535</td>\n",
       "      <td>0.825823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9    ...        cont5  \\\n",
       "0   4    A    B    A    A    A    A    A    A    B    ...     0.281143   \n",
       "1   6    A    B    A    B    A    A    A    A    B    ...     0.836443   \n",
       "2   9    A    B    A    B    B    A    B    A    B    ...     0.718531   \n",
       "3  12    A    A    A    A    B    A    A    A    A    ...     0.397069   \n",
       "4  15    B    A    A    A    A    B    A    A    A    ...     0.302678   \n",
       "\n",
       "      cont6     cont7    cont8    cont9   cont10    cont11    cont12  \\\n",
       "0  0.466591  0.317681  0.61229  0.34365  0.38016  0.377724  0.369858   \n",
       "1  0.482425  0.443760  0.71330  0.51890  0.60401  0.689039  0.675759   \n",
       "2  0.212308  0.325779  0.29758  0.34365  0.30529  0.245410  0.241676   \n",
       "3  0.369930  0.342355  0.40028  0.33237  0.31480  0.348867  0.341872   \n",
       "4  0.398862  0.391833  0.23688  0.43731  0.50556  0.359572  0.352251   \n",
       "\n",
       "     cont13    cont14  \n",
       "0  0.704052  0.392562  \n",
       "1  0.453468  0.208045  \n",
       "2  0.258586  0.297232  \n",
       "3  0.592264  0.555955  \n",
       "4  0.301535  0.825823  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = loadData(datadir,'train.csv.zip')\n",
    "display(data.info())\n",
    "display(data.head(5))\n",
    "\n",
    "test_data= loadData(datadir,'test.csv.zip') \n",
    "display(test_data.info())\n",
    "display(test_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', 188318)\n",
      "('test:', 125546)\n",
      "('combined:', 313864)\n",
      "Pre-Processing done\n",
      "('data:', 188318)\n",
      "('labels:', 188318)\n",
      "('test:', 125546)\n"
     ]
    }
   ],
   "source": [
    "# combine the two frames so we can encode the labels!\n",
    "test_data['loss']=0\n",
    "\n",
    "lengthofData=len(data)\n",
    "lengthoftest_data=len(test_data)\n",
    "\n",
    "print(\"data:\",lengthofData)\n",
    "print(\"test:\",lengthoftest_data)\n",
    "\n",
    "combineddata=pd.concat([data,test_data])\n",
    "lengthofcombined=len(combineddata)\n",
    "print(\"combined:\",lengthofcombined)\n",
    "\n",
    "# the categorical data that we need in a number format\n",
    "combineddata=LabelEncoder(combineddata)\n",
    "\n",
    "# time to split the data back apart!\n",
    "data=combineddata.iloc[:lengthofData].copy()\n",
    "test_data=combineddata.iloc[lengthofData:].copy()\n",
    "test_data.drop(['loss'],1,inplace=True) # didn't have this column before, make it go away!\n",
    "\n",
    "\n",
    "x_test = test_data.copy()\n",
    "x_test.drop(['id'],1,inplace=True)\n",
    "\n",
    "# we don't want the ID columns in X, and of course not loss either\n",
    "x=data.drop(['id','loss'],1)\n",
    "# loss is our label\n",
    "y=data['loss']\n",
    "\n",
    "#minmax scaler\n",
    "scaler= MinMaxScaler() \n",
    "x = scaler.fit_transform(x)\n",
    "x_test_data = scaler.fit_transform(x_test)\n",
    "\n",
    "#display(x[:5])\n",
    "#display(y.head(5))\n",
    "\n",
    "print(\"Pre-Processing done\")\n",
    "print(\"data:\",len(x))\n",
    "print(\"labels:\",len(y))\n",
    "print(\"test:\",len(x_test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick our sklearn regressors, and do some param optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      " KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')]\n",
      "[{'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [2, 5, 10, 50]}\n",
      " {'alpha': [0.5, 1, 2, 4, 40, 400]}\n",
      " {'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [5, 10, 50]}\n",
      " {'n_neighbors': [2, 5, 7], 'leaf_size': [3, 15, 30, 50]}]\n",
      "('number of scikitlearn regressors to use:', 4)\n"
     ]
    }
   ],
   "source": [
    "regressor_w_grid=[] # a list of regressions to use\n",
    "#regrList.append([LinearRegression()])\n",
    "regressor_w_grid.append([ExtraTreesRegressor(n_jobs = -1),\n",
    "                         dict(n_estimators=[2,5,10,50],\n",
    "                         max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([Ridge(),\n",
    "                         dict(alpha=[.5,1,2,4,40,400])])\n",
    "regressor_w_grid.append([RandomForestRegressor(#criterion = 'mae',\n",
    "                                      n_jobs =-1, \n",
    "                                      random_state=42),\n",
    "                        dict(n_estimators=[5,10,50],\n",
    "                             max_features=['auto','sqrt','log2'])])\n",
    "regressor_w_grid.append([KNeighborsRegressor(n_jobs = -1),\n",
    "                        dict(n_neighbors=[2,5,7],\n",
    "                             leaf_size =[3,15,30,50])])\n",
    "#regrList.append([SVR()]) # oh my so slow! and bad initial scores\n",
    "\n",
    "\n",
    "\n",
    "regrList=np.array(regressor_w_grid).T[0]\n",
    "paramater_grid=np.array(regressor_w_grid).T[1]\n",
    "print regrList\n",
    "print paramater_grid\n",
    "\n",
    "print(\"number of scikitlearn regressors to use:\",len(regrList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('In:', ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False))\n"
     ]
    }
   ],
   "source": [
    "#  train/validation split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split( x,\n",
    "                                                                y,\n",
    "                                                                test_size=0.35,\n",
    "                                                                random_state=42)\n",
    "for i in range(len(regrList)):\n",
    "    print(\"In:\",regrList[i])\n",
    "    #search the param_grid for best params based on the f1 score\n",
    "    grid_search = GridSearchCV(regrList[i], param_grid= paramater_grid[i], n_jobs= 1, scoring=make_scorer(mean_absolute_error)) \n",
    "    grid_search.fit(X_train,y_train)\n",
    "    #reach into the grid search and pull out the best parameters, and set those on the clf\n",
    "    params={}\n",
    "    for p in grid_search.best_params_:\n",
    "        params[p]=grid_search.best_params_[p]\n",
    "    regrList[i].set_params(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Layer 1, train and predict for layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into k-folds(divisions). train the regressors on each combination of k-1 folds, and then predict on the held-out fold. Preserve the prediction of each regressor for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data: 188318\n",
      "folds at: [(0, 37663), (37663, 75326), (75326, 112989), (112989, 150652), (150652, 188318)]\n",
      "fold size: 37663\n",
      "train size: 150652\n",
      "188318\n"
     ]
    }
   ],
   "source": [
    "#prepare the fold divisions\n",
    "\n",
    "data_size=x.shape[0]\n",
    "print \"size of train data:\",data_size\n",
    "folds=[]\n",
    "num_folds=5\n",
    "fold_start=0\n",
    "for k in range(num_folds-1):\n",
    "    fold_end=((data_size/num_folds)*(k+1))\n",
    "    folds.append((fold_start,fold_end))\n",
    "    fold_start=fold_end\n",
    "folds.append((fold_start,data_size))\n",
    "print \"folds at:\",folds\n",
    "print \"fold size:\", (data_size/num_folds)\n",
    "print \"train size:\",(data_size/num_folds)*(num_folds-1)\n",
    "\n",
    "count=0\n",
    "for i in folds:\n",
    "    count+=i[1]-i[0]\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:0 to 37663 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "\n",
      "fit time:24.432s\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.222s\n",
      "Mean abs error: 1328.95\n",
      "Score: 0.46\n",
      "\n",
      "fit time:3.594s\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.012s\n",
      "Mean abs error: 1335.61\n",
      "Score: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:24: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:27.31s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "predict time:0.215s\n",
      "Mean abs error: 1328.68\n",
      "Score: 0.46\n",
      "\n",
      "fit time:84.573s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "predict time:321.238s\n",
      "Mean abs error: 1374.38\n",
      "Score: 0.40\n",
      "\n",
      "fit time:5163.596s\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "predict time:1134.759s\n",
      "Mean abs error: 1637.49\n",
      "Score: 0.05\n",
      "--layer2 length: 37663\n",
      "--layer2 shape: (37663, 5)\n",
      "Fold run time:8229.209s\n",
      "Fold:37663 to 75326 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "\n",
      "fit time:24.774s\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.226s\n",
      "Mean abs error: 1313.48\n",
      "Score: 0.48\n",
      "\n",
      "fit time:3.553s\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.011s\n",
      "Mean abs error: 1322.17\n",
      "Score: 0.48\n",
      "\n",
      "fit time:26.836s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "predict time:0.223s\n",
      "Mean abs error: 1302.42\n",
      "Score: 0.49\n",
      "\n",
      "fit time:82.543s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "predict time:322.912s\n",
      "Mean abs error: 1371.71\n",
      "Score: 0.38\n",
      "\n",
      "fit time:5225.163s\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "predict time:1157.725s\n",
      "Mean abs error: 1632.05\n",
      "Score: 0.05\n",
      "--layer2 length: 75326\n",
      "--layer2 shape: (75326, 5)\n",
      "Fold run time:8339.506s\n",
      "Fold:75326 to 112989 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:51: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fit time:24.63s\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.216s\n",
      "Mean abs error: 1322.61\n",
      "Score: 0.48\n",
      "\n",
      "fit time:3.59s\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.012s\n",
      "Mean abs error: 1330.79\n",
      "Score: 0.48\n",
      "\n",
      "fit time:26.933s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "predict time:0.242s\n",
      "Mean abs error: 1317.28\n",
      "Score: 0.48\n",
      "\n",
      "fit time:87.504s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "predict time:300.184s\n",
      "Mean abs error: 1392.41\n",
      "Score: 0.38\n",
      "\n",
      "fit time:5186.138s\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "predict time:1148.006s\n",
      "Mean abs error: 1644.27\n",
      "Score: 0.04\n",
      "--layer2 length: 112989\n",
      "--layer2 shape: (112989, 5)\n",
      "Fold run time:8230.359s\n",
      "Fold:112989 to 150652 of: 188318\n",
      "\n",
      "folding! len test 37663, len train 150655\n",
      "\n",
      "fit time:24.663s\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.227s\n",
      "Mean abs error: 1323.80\n",
      "Score: 0.49\n",
      "\n",
      "fit time:3.476s\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.011s\n",
      "Mean abs error: 1338.83\n",
      "Score: 0.48\n",
      "\n",
      "fit time:27.148s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "predict time:0.241s\n",
      "Mean abs error: 1323.89\n",
      "Score: 0.49\n",
      "\n",
      "fit time:86.128s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "predict time:300.462s\n",
      "Mean abs error: 1389.29\n",
      "Score: 0.39\n",
      "\n",
      "fit time:5220.711s\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "predict time:1172.927s\n",
      "Mean abs error: 1643.64\n",
      "Score: 0.04\n",
      "--layer2 length: 150652\n",
      "--layer2 shape: (150652, 5)\n",
      "Fold run time:8321.336s\n",
      "Fold:150652 to 188318 of: 188318\n",
      "\n",
      "folding! len test 37666, len train 150652\n",
      "\n",
      "fit time:26.592s\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.227s\n",
      "Mean abs error: 1310.15\n",
      "Score: 0.49\n",
      "\n",
      "fit time:3.542s\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.012s\n",
      "Mean abs error: 1326.81\n",
      "Score: 0.48\n",
      "\n",
      "fit time:29.975s\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n",
      "predict time:0.215s\n",
      "Mean abs error: 1311.63\n",
      "Score: 0.50\n",
      "\n",
      "fit time:86.546s\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "predict time:320.7s\n",
      "Mean abs error: 1367.27\n",
      "Score: 0.39\n",
      "\n",
      "fit time:5247.815s\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "predict time:1168.886s\n",
      "Mean abs error: 1623.22\n",
      "Score: 0.05\n",
      "--layer2 length: 188318\n",
      "--layer2 shape: (188318, 5)\n",
      "Fold run time:8390.323s\n",
      "Full run time:41510.735s\n"
     ]
    }
   ],
   "source": [
    "x_layer2=[]\n",
    "start_time0 = time.time()\n",
    "MAE_tracking=[]\n",
    "\n",
    "for fold_start,fold_end in folds:\n",
    "    print(\"Fold:{} to {} of: {}\".format(fold_start,fold_end,data_size))\n",
    "    start_time1 = time.time()\n",
    "    fold_result=[]\n",
    "    \n",
    "    X_test = x[fold_start:fold_end].copy()\n",
    "    y_test = y[fold_start:fold_end].copy()\n",
    "    X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n",
    "    y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n",
    "    print \"\\nfolding! len test {}, len train {}\".format(len(X_test),len(X_train))\n",
    "    \n",
    "    for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "        start_time = time.time()\n",
    "        regrList[i].fit(X_train,y_train)\n",
    "        print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "        start_time = time.time()\n",
    "        print(regrList[i])\n",
    "        curr_predict=regrList[i].predict(X_test)\n",
    "        if fold_result == []:\n",
    "            fold_result = np.array(curr_predict.copy())\n",
    "        else:\n",
    "            fold_result = np.column_stack((fold_result,curr_predict))\n",
    "        \n",
    "        print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "        #show some stats on that last regressions run\n",
    "        MAE=np.mean(abs(curr_predict - y_test))\n",
    "        MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,i),MAE])\n",
    "        print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "        print(\"Score: {:.2f}\".format(regrList[i].score(X_test, y_test)))\n",
    "    \n",
    "    #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "    \n",
    "    if use_xgb == True:\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "        gbdt=xgbfit(X_train,y_train)\n",
    "\n",
    "        # now do a prediction and spit out a score(MAE) that means something\n",
    "        start_time = time.time()\n",
    "        curr_predict=gbdt.predict(dtest)\n",
    "        fold_result = np.column_stack((fold_result,curr_predict))   \n",
    "        MAE=np.mean(abs(curr_predict - y_test))\n",
    "        MAE_tracking.append([\"run:{}-{}:{}\".format(fold_start,fold_end,'XGB'),MAE])\n",
    "        print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "        print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "    if x_layer2 == []:\n",
    "        x_layer2=fold_result\n",
    "    else:\n",
    "        x_layer2=np.append(x_layer2,fold_result,axis=0)\n",
    "        \n",
    "    print \"--layer2 length:\",len(x_layer2)\n",
    "    print \"--layer2 shape:\",np.shape(x_layer2)\n",
    "    print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \n",
    "print(\"Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### train layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "Mean abs error: 1234.98\n",
      "Score: 0.52\n"
     ]
    }
   ],
   "source": [
    "print len(x_layer2)\n",
    "print len(y)\n",
    "\n",
    "#  train/validation split\n",
    "X_layer2_train, X_layer2_validation, y_layer2_train, y_layer2_validation = train_test_split( x_layer2,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)\n",
    "layer2_regr=LinearRegression()\n",
    "\n",
    "layer2_regr.fit(X_layer2_train,y_layer2_train)\n",
    "\n",
    "layer2_predict=layer2_regr.predict(X_layer2_validation)\n",
    "\n",
    "#show some stats on that last regressions run    \n",
    "MAE=np.mean(abs(layer2_predict - y_layer2_validation))\n",
    "MAE_tracking.append([\"run:{}\".format('linearLayer2'),MAE])\n",
    "print(\"Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"Score: {:.2f}\".format(layer2_regr.score(X_layer2_validation, y_layer2_validation)))\n",
    "\n",
    "\n",
    "#with LinearReg: Mean abs error: 1238.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188318\n",
      "188318\n",
      "[0]\ttrain-mae:2810.86+3.89543\ttest-mae:2810.88+12.1435\n",
      "fit time:6.476s\n",
      "CV-Mean: 1201.6030885+6.95497704099\n",
      "Train time:1.386s\n",
      "XGB Mean abs error: 1199.61\n",
      "XGB predict time:0.03s\n"
     ]
    }
   ],
   "source": [
    "# The XGB version of layer 2\n",
    "print len(x_layer2)\n",
    "print len(y)\n",
    "\n",
    "#  train/validation split\n",
    "X_layer2_train, X_layer2_validation, y_layer2_train, y_layer2_validation = train_test_split( x_layer2,\n",
    "                                                                                y,\n",
    "                                                                                test_size=0.25,\n",
    "                                                                                random_state=42)\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "dtest = xgb.DMatrix(X_layer2_validation)\n",
    "layer2_gbdt=xgbfit(X_layer2_train,y_layer2_train)\n",
    "\n",
    "# now do a prediction and spit out a score(MAE) that means something\n",
    "start_time = time.time()\n",
    "MAE=np.mean(abs(layer2_gbdt.predict(dtest) - y_layer2_validation))\n",
    "MAE_tracking.append([\"run:{}\".format('XGBLayer2'),MAE])\n",
    "print(\"XGB Mean abs error: {:.2f}\".format(MAE))\n",
    "print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "#with LinearReg: XGB Mean abs error: 1205.77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['run:0-37663:0' 'run:0-37663:1' 'run:0-37663:2' 'run:0-37663:3'\n",
      "  'run:0-37663:4' 'run:37663-75326:0' 'run:37663-75326:1'\n",
      "  'run:37663-75326:2' 'run:37663-75326:3' 'run:37663-75326:4'\n",
      "  'run:75326-112989:0' 'run:75326-112989:1' 'run:75326-112989:2'\n",
      "  'run:75326-112989:3' 'run:75326-112989:4' 'run:112989-150652:0'\n",
      "  'run:112989-150652:1' 'run:112989-150652:2' 'run:112989-150652:3'\n",
      "  'run:112989-150652:4' 'run:150652-188318:0' 'run:150652-188318:1'\n",
      "  'run:150652-188318:2' 'run:150652-188318:3' 'run:150652-188318:4'\n",
      "  'run:linearLayer2' 'run:XGBLayer2']\n",
      " ['1328.94587178' '1335.6109609' '1328.67511744' '1374.37524594'\n",
      "  '1637.49060539' '1313.47777705' '1322.16612723' '1302.41998256'\n",
      "  '1371.70692547' '1632.05075006' '1322.61166434' '1330.79485737'\n",
      "  '1317.28013905' '1392.41162106' '1644.26719563' '1323.79915295'\n",
      "  '1338.83235314' '1323.88590739' '1389.28851228' '1643.64342547'\n",
      "  '1310.1490114' '1326.80644028' '1311.62821712' '1367.2665912'\n",
      "  '1623.22081518' '1234.98054717' '1199.60713353']]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAFsCAYAAAAudtVFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npcjOIJVKpVJkZ5BKpVIpsjNIpVKpFPD/P0ucIVhtRRwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb110bfec90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "MAE_tracking_graph=np.array(MAE_tracking)\n",
    "\n",
    "print(MAE_tracking_graph.T)\n",
    "\n",
    "plt.plot(MAE_tracking_graph.T[1])\n",
    "plt.xlabel(MAE_tracking_graph.T[0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del MAE_tracking_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict layer 1 on test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "          verbose=0, warm_start=False)\n",
      "predict time:0.997s\n",
      "[ 2019.663  1887.378  9884.11  ...,  2837.68   1271.029  4126.829]\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "predict time:0.078s\n",
      "[  1155.44633825   1986.3797948   11490.70006385 ...,   2823.69355514\n",
      "   1076.85379674   4646.9250013 ]\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=-1, oob_score=False, random_state=42,\n",
      "           verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:9: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict time:0.775s\n",
      "[ 2205.797  2107.822  8917.102 ...,  3224.901  1168.459  4035.305]\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
      "          weights='uniform')\n",
      "predict time:1054.801s\n",
      "[  2031.318   2996.604  12786.978 ...,   2584.786    718.254   2704.898]\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "predict time:3896.578s\n",
      "[ 2172.06914416  2404.85011431  3213.6472017  ...,  2337.20482649\n",
      "  1735.90006022  2597.69942176]\n",
      "Fold run time:4953.24s\n"
     ]
    }
   ],
   "source": [
    "x_layer2_test = []\n",
    "start_time1 = time.time()\n",
    "for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n",
    "    start_time = time.time()\n",
    "    print(regrList[i])\n",
    "    curr_predict=regrList[i].predict(x_test_data)\n",
    "    print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "    \n",
    "    if x_layer2_test == []:\n",
    "        x_layer2_test = np.array(curr_predict.copy())\n",
    "    else:\n",
    "        x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    print curr_predict\n",
    "\n",
    "#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n",
    "if use_xgb == True:\n",
    "    dtest = xgb.DMatrix(x_test_data)\n",
    "    # now do a prediction and spit out a score(MAE) that means something\n",
    "    #start_time = time.time()\n",
    "    curr_predict=gbdt.predict(dtest)\n",
    "    x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n",
    "    #print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\n",
    "    print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n",
    "\n",
    "print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'size of original test data:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "125546"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Test shape:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(125546, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'train shape:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(188318, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sample of layer2 test:\\n', array([[  2019.663     ,   1155.44633825,   2205.797     ,   2031.318     ,\n",
      "          2172.06914416],\n",
      "       [  1887.378     ,   1986.3797948 ,   2107.822     ,   2996.604     ,\n",
      "          2404.85011431],\n",
      "       [  9884.11      ,  11490.70006385,   8917.102     ,  12786.978     ,\n",
      "          3213.6472017 ],\n",
      "       [  6694.445     ,   5737.1042642 ,   4967.937     ,   5496.916     ,\n",
      "          2539.96079807]]))\n",
      "('x_layer2_test mean:', array([ 3098.77643271,  3056.02085022,  3087.85538827,  2802.74731904,\n",
      "        2188.77335912]))\n",
      "('x_layer2 mean:', array([ 3092.88849374,  3037.45612893,  3093.94178225,  2814.07853901,\n",
      "        2188.66433305]))\n",
      "('x_layer2_test std:', array([ 2236.9998565 ,  2016.1297939 ,  2242.43405966,  1904.52404511,\n",
      "         396.56923677]))\n",
      "('x_layer2 std:', array([ 2260.99674164,  2017.89115492,  2266.12894136,  1929.42056015,\n",
      "         397.17504012]))\n",
      "('num outliers:', 6)\n",
      "3092.88849374\n",
      "('num outliers:', 0)\n",
      "3092.88849374\n"
     ]
    }
   ],
   "source": [
    "# some problems noted---fact finding below!\n",
    "display(\"size of original test data:\",len(x_test_data))\n",
    "display(\"Test shape:\",np.shape(x_layer2_test))\n",
    "display(\"train shape:\",np.shape(x_layer2))\n",
    "\n",
    "print(\"sample of layer2 test:\\n\",x_layer2_test[:4])\n",
    "\n",
    "print(\"x_layer2_test mean:\",x_layer2_test.mean( axis=0))\n",
    "print(\"x_layer2 mean:\",x_layer2.mean(axis=0))\n",
    "train_layer2_col0_mean=x_layer2.mean(axis=0)[0]\n",
    "\n",
    "print(\"x_layer2_test std:\",x_layer2_test.std( axis=0)) \n",
    "print(\"x_layer2 std:\",x_layer2.std(axis=0))\n",
    "\n",
    "# notice that column 0(linregresion) has a significantly higher mean and std\n",
    "# here's a hack to not fix that for now! \n",
    "\n",
    "# check which row in column 0 are significantly far from the mean\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "#for each problem child, set them to the average value from the train set, to null the affect some\n",
    "for o in outliers:\n",
    "    problem_column[o[0]]=train_layer2_col0_mean\n",
    "    \n",
    "print(problem_column[o[0]])\n",
    "\n",
    "#check outliers again\n",
    "problem_column=x_layer2_test.T[0]\n",
    "outliers=[]\n",
    "for i in range(len(problem_column)):\n",
    "    if problem_column[i]>30000:\n",
    "        outliers.append((i,problem_column[i]))\n",
    "print(\"num outliers:\",len(outliers))\n",
    "\n",
    "print(x_layer2_test.T[0][o[0]]) # verify that the change made it all the way to the original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,1701.59760295\\n',\n",
       " '6,1973.73446861\\n',\n",
       " '9,10723.8506793\\n',\n",
       " '12,5821.67911105\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data['loss']=layer2_regr.predict(x_layer2_test)\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,loss\\n',\n",
       " '4,1668.34204102\\n',\n",
       " '6,1899.40551758\\n',\n",
       " '9,9355.84570312\\n',\n",
       " '12,5404.53564453\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the XGB version:\n",
    "dtest = xgb.DMatrix(x_layer2_test)\n",
    "test_data['loss']=layer2_gbdt.predict(dtest)\n",
    "\n",
    "result=test_data[['id','loss',]]\n",
    "output_fname=\"result_submission_stack_xgb.csv\"\n",
    "display(writeData(result,output_fname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('result std:', id      170098.328125\n",
      "loss      1873.819946\n",
      "dtype: float32)\n"
     ]
    }
   ],
   "source": [
    "#let's have a look at the std of the result, as a cross check\n",
    "print(\"result std:\",result.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOF "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
