{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "Just a simple linear regression with scikit. Well, that's how it started\n\nok, not so simple. I seem to keep adding junk.  currently using a random forest, and plan to add in some nice gridsearch, etc.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os,sys,time,random,math\nimport tarfile, zipfile\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import decomposition, datasets, ensemble\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, Image\n\nimport xgboost as xgb\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))",
   "execution_count": null,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def loadData(datadir,filename):\n    # Load the wholesale customers dataset\n    #data = pd.read_csv(filename)\n    data = ''\n    print (\"loading: \"+datadir+filename)\n    try:\n        if zipfile.is_zipfile(datadir+filename):\n            z = zipfile.ZipFile(datadir+filename)\n            filename = z.open(filename[:-4])\n        else:\n            filename=datadir+filename\n        data = pd.read_csv(filename, parse_dates=True)  \n        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n    except Exception as e:\n        print (\"Dataset could not be loaded. Is the dataset missing?\")\n        print(e)\n    return data\n\ndef writeData(data,filename):\n    # Load the wholesale customers dataset\n    try:\n        data.to_csv(filename, index=False)\n    except Exception as e:\n        print (\"Dataset could not be written.\")\n        print(e)\n    verify=[]\n    try:\n        with open(filename, 'r') as f:\n            for line in f:\n                verify.append(line)\n        f.closed\n        return verify[:5]\n    except IOError:\n        sys.std",
   "execution_count": null,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "datadir=\"../input/\"\ndata = loadData(datadir,'train.csv')\ndisplay(data.info())\ndisplay(data.head(5))",
   "execution_count": null,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "features = data.columns\ncats = [feat for feat in features if 'cat' in feat]\nfor feat in cats:\n    data[feat] = pd.factorize(data[feat], sort=True)[0]\n\ndisplay(data.info())\ndisplay(data.head())",
   "execution_count": null,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\nx=data.drop(['id','loss'],1).fillna(value=0)\ny=data['loss']\n\ndisplay(x.head(5))\ndisplay(y.head(5))",
   "execution_count": null,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#  train/validation split\nX_train, X_test, y_train, y_test = train_test_split( x.values, y.values, test_size=0.25, random_state=42)\n\ndataSize=X_train.shape[0]\nprint (\"size of train data\",dataSize, )\ntest_sizes=[50]\nfor i in range(5):\n    test_sizes.append(int(round(dataSize*(i+1)*.2)))\n\nprint (\"run tests of size\",test_sizes)",
   "execution_count": null,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "regrList=[]\nregrList.append(LinearRegression())\n#regrList.append(SVR()) #long run time!\nregrList.append(ExtraTreesRegressor())\nregrList.append(RandomForestRegressor())\n#regrList.append(ensemble.AdaBoostRegressor())  ## The error rate is the bad!\n\n#regrList.append(xgb.XGBClassifier(max_depth=6, learning_rate=0.075, n_estimators=15,\n#                                objective=\"reg:linear\", subsample=0.7,\n#                                colsample_bytree=0.7, seed=42))\n\n#pca = decomposition.PCA(n_components = 100)\n#regr = Pipeline(steps=[('pca', pca), ('classifier', regr )]) # set up the clf as a pipeline so we can do randomized PCA\n\n#params=dict(fit_intercept=[True,False], normalize  = [True,False])\n#grid_search = GridSearchCV(regr, param_grid= params, n_jobs= 1, scoring=make_scorer(f1_score)) \n#grid_search.fit(X_train,y_train)\n\nfor i in range(len(regrList)):\n                regrList[i].fit(X_train ,y_train )",
   "execution_count": null,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "for i in range(len(regrList)):\n    print(regrList[i])\n    print(\"Mean abs error: {:.2f}\".format(np.mean(abs(regrList[i].predict(X_test) - y_test))))\n    print(\"Score: {:.2f}\".format(regrList[i].score(X_test, y_test)))",
   "execution_count": null,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# refit the full train data!\nfor i in range(len(regrList)):\n    regrList[i].fit(x ,y )\n    print(\"Mean abs error: {:.2f}\".format(np.mean(abs(regrList[i].predict(X_test) - y_test))))\n    print(\"Score: {:.2f}\".format(regrList[i].score(X_test, y_test)))",
   "execution_count": null,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": null,
   "execution_count": null,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# predict the test data!\ntest_data= loadData(datadir,'test.csv')\ndisplay(test_data.info())\ndisplay(test_data.head(5))\n\nfeatures = test_data.columns\ncats = [feat for feat in features if 'cat' in feat]\nfor feat in cats:\n    test_data[feat] = pd.factorize(test_data[feat], sort=True)[0]\n\ntest_X=test_data.drop(['id'],1).fillna(value=0)\ntest_data['loss']=0\nfor i in range(len(regrList)): \n    test_data['loss']=test_data['loss']+regrList[i].predict(test_X) \ntest_data['loss']=test_data['loss']/(i+1)\ndisplay(test_data.info())\ndisplay(test_data.head())\n\nresult=test_data[['id','loss']]\ndisplay(result.info())\ndisplay(result.head())",
   "execution_count": null,
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "output_fname=\"result_submission.csv\"\nwriteData(result,output_fname)",
   "execution_count": null,
   "outputs": [],
   "metadata": {}
  }
 ]
}